# 声明组件: source 和 channel. 不需要 sink
a1.sources = r1
a1.channels = c1 c2


# =========配置 source
a1.sources.r1.type = TAILDIR
# 监控的文件组
a1.sources.r1.filegroups = f1
# 给每个文件组置文件
a1.sources.r1.filegroups.f1=/tmp/logs/app.+
# 记住传输的位置, 断点续传使用
a1.sources.r1.positionFile=/opt/module/flume-1.7.0/taildir_position.json

# 配置 channel Selector
a1.sources.r1.selector.type = multiplexing
# 需要在拦截器中完成添加 header 的工作
a1.sources.r1.selector.header = topic
a1.sources.r1.selector.mapping.topic_start = c1
a1.sources.r1.selector.mapping.topic_event = c2

# ========配置拦截器: 两个拦截器 1. ETL 2. 对事件分类
a1.sources.r1.interceptors = i1 i2
# etl
a1.sources.r1.interceptors.i1.type = com.atguigu.dw.flume.interceptor.LogETLInterceptor$Builder
# 对事件分类
a1.sources.r1.interceptors.i2.type = com.atguigu.dw.flume.interceptor.LogTypeInterceptor$Builder

# =======配置 channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = hadoop201:9092,hadop202:9092,hadoop203:9092
a1.channels.c1.kafka.topic = topic_start
# 应该配置 false  表示是否需要把 event 的结构放入到的 kafka 的 topic 中
a1.channels.c1.parseAsFlumeEvent = false


a1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c2.kafka.bootstrap.servers = hadoop201:9092,hadop202:9092,hadoop203:9092
a1.channels.c2.kafka.topic = topic_event
# 应该配置 false
a1.channels.c2.parseAsFlumeEvent = false


# ========== 组装组件 ( 只需要连接 source->channel  没有 sink 所以无需 channel->sink )
a1.sources.r1.channels = c1 c2

