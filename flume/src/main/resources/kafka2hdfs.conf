# r1 -> c1 -> k1
# r2 -> c2 -> k2

# 声明组件 开始
a1.sources = r1 r2
a1.channels = c1 c2
a1.sinks = k1 k2
# 声明组件 结束

# ====== 配置 sources 开始
## 配置 r1
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r1.kafka.topics = topic_start
      # 一个批次向 channel 写入的最大消息数
a1.sources.r1.batchSize = 5000
      # 一个批次的数据最长的等待时间
a1.sources.r1.batchDurationMillis = 2000
## 配置 r2
a1.sources.r2.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r2.kafka.topics = topic_event
    # 一个批次向 channel 写入的最大消息数
a1.sources.r2.batchSize = 5000
    # 一个批次的数据最长的等待时间
a1.sources.r2.batchDurationMillis = 2000
# 配置 sources 结束

# ====== 配置 channels 开始
## 配置 c1
a1.channels.c1.type = file
    # 数据存储的位置
a1.channels.c1.dataDirs = /opt/module/flume-1.7.0/checkpoint/behavior1
    # 记录 sink 读取的位置
a1.channels.c1.checkpointDir = /opt/module/flume-1.7.0/data/behavior1
    # 单个文件大小的最大值
a1.channels.c1.maxFileSize = 2146435071
    # channel 的容量
a1.channels.c1.capacity = 1000000
    # 等待put操作的总时间
a1.channels.c1.keep-alive = 6
## 配置 c2
a1.channels.c1.type = file
    # 数据存储的位置
a1.channels.c1.dataDirs = /opt/module/flume-1.7.0/checkpoint/behavior2
    # 记录 sink 读取的位置
a1.channels.c1.checkpointDir = /opt/module/flume-1.7.0/data/behavior2
    # 单个文件大小的最大值
a1.channels.c1.maxFileSize = 2146435071
    # channel 的容量
a1.channels.c1.capacity = 1000000
    # 等待put操作的总时间
a1.channels.c1.keep-alive = 6
# 配置 channels  结束


# ===== 配置 sinks 开始
## 配置 k1
a1.sinks.k1.type = hdfs
  # 每天一个文件文件夹
a1.sinks.k1.hdfs.path = hdfs://hadoop102:9000/origin_data/gmall/log/topic_start/%Y-%m-%d
  # 日志文件前缀
a1.sinks.k1.hdfs.filePrefix = logstart-
  # 影响所有基于时间的选项例如: %Y,....
a1.sinks.k2.hdfs.round = true
a1.sinks.k2.hdfs.roundValue = 10
a1.sinks.k2.hdfs.roundUnit = second
## 配置 k2
a1.sinks.k2.type = hdfs
  # 每天一个文件文件夹
a1.sinks.k2.hdfs.path = hdfs://hadoop102:9000/origin_data/gmall/log/topic_event/%Y-%m-%d
  # 日志文件前缀
a1.sinks.k2.hdfs.filePrefix = logevent-
  # 影响所有基于时间的选项例如: %Y,....   咱们的案例无效, 应该设置为 false
a1.sinks.k2.hdfs.round = true
a1.sinks.k2.hdfs.roundValue = 10
a1.sinks.k2.hdfs.roundUnit = second

## 不要产生大量小文件
a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

a1.sinks.k2.hdfs.rollInterval = 10
a1.sinks.k2.hdfs.rollSize = 134217728
a1.sinks.k2.hdfs.rollCount = 0

## 控制输出文件是否原生文件。
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k2.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = lzop
a1.sinks.k2.hdfs.codeC = lzop
# 配置 sinks 结束

# 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1

a1.sources.r2.channels = c2
a1.sinks.k2.channel= c2
